{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-14T05:54:03.643728Z",
     "iopub.status.busy": "2024-07-14T05:54:03.642989Z",
     "iopub.status.idle": "2024-07-14T05:54:03.669890Z",
     "shell.execute_reply": "2024-07-14T05:54:03.668833Z",
     "shell.execute_reply.started": "2024-07-14T05:54:03.643695Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Paths to the CSV files and image directories\n",
    "csv_paths = {\n",
    "    'Train': 'G:\\\\new dataset\\\\Disaster_train.csv',\n",
    "    'Test': 'G:\\\\new dataset\\\\Disaster_test.csv',\n",
    "    'Validation': 'G:\\\\new dataset\\\\Disaster_Validation.csv'\n",
    "}\n",
    "\n",
    "image_dirs = {\n",
    "    'Train': 'G:\\\\new dataset\\\\Train',\n",
    "    'Test': 'G:\\\\new dataset\\\\Test',\n",
    "    'Validation': 'G:\\\\new dataset\\\\Validation'\n",
    "}\n",
    "\n",
    "output_dir = 'G:\\\\new dataset\\\\Output'  # Output directory to save the CSV files\n",
    "\n",
    "# Function to check for matching Meme_ID and image files, and add image paths\n",
    "def check_matches(csv_path, image_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    image_files = os.listdir(image_dir)\n",
    "    image_names = {os.path.splitext(image_file)[0]: os.path.join(image_dir, image_file) for image_file in image_files}\n",
    "    \n",
    "    # Add Image_Path column to the dataframe\n",
    "    df['Image_Path'] = df['image_id'].apply(lambda x: image_names.get(x, None))\n",
    "    \n",
    "    # Filter rows where Image_Path is not None (i.e., matched Meme_IDs)\n",
    "    matched_df = df[df['Image_Path'].notna()]\n",
    "    \n",
    "    return matched_df\n",
    "\n",
    "# Check matches for each set (Train, Test, Validation)\n",
    "for key in csv_paths:\n",
    "    matched_df = check_matches(csv_paths[key], image_dirs[key])\n",
    "    \n",
    "    matches_output_path = os.path.join(output_dir, f'{key}_matches.csv')\n",
    "    \n",
    "    matched_df.to_csv(matches_output_path, index=False)\n",
    "    \n",
    "    print(f\"{key} set:\")\n",
    "    print(f\"Matched Meme_IDs with image paths saved to {matches_output_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T05:54:06.709525Z",
     "iopub.status.busy": "2024-07-14T05:54:06.708920Z",
     "iopub.status.idle": "2024-07-14T05:54:06.723290Z",
     "shell.execute_reply": "2024-07-14T05:54:06.722545Z",
     "shell.execute_reply.started": "2024-07-14T05:54:06.709494Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('G:\\\\new dataset\\\\Output\\\\Train_matches.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T05:54:09.534705Z",
     "iopub.status.busy": "2024-07-14T05:54:09.534047Z",
     "iopub.status.idle": "2024-07-14T05:54:09.547756Z",
     "shell.execute_reply": "2024-07-14T05:54:09.546757Z",
     "shell.execute_reply.started": "2024-07-14T05:54:09.534672Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('G:\\\\new dataset\\\\Output\\\\Test_matches.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T05:54:11.550641Z",
     "iopub.status.busy": "2024-07-14T05:54:11.549795Z",
     "iopub.status.idle": "2024-07-14T05:54:11.562839Z",
     "shell.execute_reply": "2024-07-14T05:54:11.562032Z",
     "shell.execute_reply.started": "2024-07-14T05:54:11.550610Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_df = pd.read_csv('G:\\\\new dataset\\\\Output\\\\Validation_matches.csv')\n",
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T05:54:13.353958Z",
     "iopub.status.busy": "2024-07-14T05:54:13.353202Z",
     "iopub.status.idle": "2024-07-14T05:54:13.358986Z",
     "shell.execute_reply": "2024-07-14T05:54:13.357945Z",
     "shell.execute_reply.started": "2024-07-14T05:54:13.353928Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display the sizes of each split\n",
    "print(f\"Train dataset size: {len(train_df)}\")\n",
    "print(f\"Test dataset size: {len(test_df)}\")\n",
    "print(f\"Validation dataset size: {len(validation_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T05:54:15.444844Z",
     "iopub.status.busy": "2024-07-14T05:54:15.444480Z",
     "iopub.status.idle": "2024-07-14T05:54:15.482417Z",
     "shell.execute_reply": "2024-07-14T05:54:15.481406Z",
     "shell.execute_reply.started": "2024-07-14T05:54:15.444816Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Function to remove punctuation (preserve Bangla characters)\n",
    "def remove_punctuation(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Function to remove extra whitespace\n",
    "def remove_whitespace(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "# Function to remove emojis\n",
    "def remove_emojis(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Function to remove URLs\n",
    "def remove_urls(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "# Function to remove HTML tags\n",
    "def remove_html(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    html_pattern = re.compile(r'<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "# Function to remove special characters (preserve Bangla characters)\n",
    "def remove_special_characters(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return re.sub(r'[^A-Za-z0-9\\s\\u0980-\\u09FF]', '', text)\n",
    "\n",
    "# Combine all cleaning functions\n",
    "def clean_text(text):\n",
    "    text = remove_urls(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_whitespace(text)\n",
    "    return text\n",
    "\n",
    "# Mapping categories to integers\n",
    "category_mapping = {\n",
    "    'Landslides': 0,\n",
    "    'Wildfire': 1,\n",
    "    'Drought': 2,\n",
    "    'Flood': 3,\n",
    "    'Earthquake': 4,\n",
    "    'Tropical Storm': 5,\n",
    "    'Non Disaster': 6,\n",
    "    'Human Damage': 7,\n",
    "}\n",
    "\n",
    "# Load and clean the dataframes\n",
    "csv_paths = {\n",
    "    'Train': 'G:\\\\new dataset\\\\Output\\\\Train_matches.csv',\n",
    "    'Test': 'G:\\\\new dataset\\\\Output\\\\Test_matches.csv',\n",
    "    'Validation': 'G:\\\\new dataset\\\\Output\\\\Validation_matches.csv'\n",
    "}\n",
    "\n",
    "cleaned_output_paths = {\n",
    "    'Train': 'G:\\\\new dataset\\\\Output\\\\Train_matches_cleaned.csv',\n",
    "    'Test': 'G:\\\\new dataset\\\\Output\\\\Test_matches_cleaned.csv',\n",
    "    'Validation': 'G:\\\\new dataset\\\\Output\\\\Validation_matches_cleaned.csv'\n",
    "}\n",
    "\n",
    "text_columns = ['context', 'category']\n",
    "\n",
    "for key in csv_paths:\n",
    "    # Load the dataframe\n",
    "    df = pd.read_csv(csv_paths[key])\n",
    "    \n",
    "    # Apply cleaning to all relevant text columns\n",
    "    for column in text_columns:\n",
    "        df[column] = df[column].astype(str).apply(clean_text)\n",
    "    \n",
    "    # Map 'category' column to integers\n",
    "    df['category'] = df['category'].map(category_mapping)\n",
    "    \n",
    "    # Add 'label' column (same as 'category' for now)\n",
    "    df['label'] = df['category']\n",
    "    \n",
    "    # Display the cleaned dataframe\n",
    "    print(f\"Cleaned {key} dataframe:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Save the cleaned dataframe to a new CSV file\n",
    "    df.to_csv(cleaned_output_paths[key], index=False)\n",
    "    print(f\"Cleaned dataframe saved to {cleaned_output_paths[key]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T05:54:20.751654Z",
     "iopub.status.busy": "2024-07-14T05:54:20.751282Z",
     "iopub.status.idle": "2024-07-14T05:54:20.765305Z",
     "shell.execute_reply": "2024-07-14T05:54:20.764424Z",
     "shell.execute_reply.started": "2024-07-14T05:54:20.751629Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('G:\\\\new dataset\\\\Output\\\\Train_matches_cleaned.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T05:54:22.912302Z",
     "iopub.status.busy": "2024-07-14T05:54:22.911952Z",
     "iopub.status.idle": "2024-07-14T05:54:22.926317Z",
     "shell.execute_reply": "2024-07-14T05:54:22.925346Z",
     "shell.execute_reply.started": "2024-07-14T05:54:22.912277Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('G:\\\\new dataset\\\\Output\\\\Test_matches_cleaned.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T05:54:24.639750Z",
     "iopub.status.busy": "2024-07-14T05:54:24.639026Z",
     "iopub.status.idle": "2024-07-14T05:54:24.652562Z",
     "shell.execute_reply": "2024-07-14T05:54:24.651568Z",
     "shell.execute_reply.started": "2024-07-14T05:54:24.639721Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_df = pd.read_csv('G:\\\\new dataset\\\\Output\\\\Validation_matches_cleaned.csv')\n",
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T05:54:26.539615Z",
     "iopub.status.busy": "2024-07-14T05:54:26.538728Z",
     "iopub.status.idle": "2024-07-14T05:54:26.544609Z",
     "shell.execute_reply": "2024-07-14T05:54:26.543653Z",
     "shell.execute_reply.started": "2024-07-14T05:54:26.539584Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T05:54:29.252983Z",
     "iopub.status.busy": "2024-07-14T05:54:29.252631Z",
     "iopub.status.idle": "2024-07-14T05:54:41.720234Z",
     "shell.execute_reply": "2024-07-14T05:54:41.719175Z",
     "shell.execute_reply.started": "2024-07-14T05:54:29.252956Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T06:03:01.582705Z",
     "iopub.status.busy": "2024-07-14T06:03:01.582300Z",
     "iopub.status.idle": "2024-07-14T06:03:06.982791Z",
     "shell.execute_reply": "2024-07-14T06:03:06.981873Z",
     "shell.execute_reply.started": "2024-07-14T06:03:01.582678Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, PoolFormerModel\n",
    "import torch\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"sail/poolformer_s12\")\n",
    "model = PoolFormerModel.from_pretrained(\"sail/poolformer_s12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T06:07:00.760292Z",
     "iopub.status.busy": "2024-07-14T06:07:00.759907Z",
     "iopub.status.idle": "2024-07-14T06:07:02.175414Z",
     "shell.execute_reply": "2024-07-14T06:07:02.174578Z",
     "shell.execute_reply.started": "2024-07-14T06:07:00.760266Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel,AdamW\n",
    "# Initialize BERT tokenizer and model\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T06:07:04.194802Z",
     "iopub.status.busy": "2024-07-14T06:07:04.194055Z",
     "iopub.status.idle": "2024-07-14T06:07:04.199952Z",
     "shell.execute_reply": "2024-07-14T06:07:04.198979Z",
     "shell.execute_reply.started": "2024-07-14T06:07:04.194772Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T06:07:05.977839Z",
     "iopub.status.busy": "2024-07-14T06:07:05.977469Z",
     "iopub.status.idle": "2024-07-14T06:07:05.982286Z",
     "shell.execute_reply": "2024-07-14T06:07:05.981405Z",
     "shell.execute_reply.started": "2024-07-14T06:07:05.977810Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Enable device-side assertions\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T06:07:07.986542Z",
     "iopub.status.busy": "2024-07-14T06:07:07.986142Z",
     "iopub.status.idle": "2024-07-14T06:07:07.996936Z",
     "shell.execute_reply": "2024-07-14T06:07:07.996032Z",
     "shell.execute_reply.started": "2024-07-14T06:07:07.986511Z"
    }
   },
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T06:07:11.000756Z",
     "iopub.status.busy": "2024-07-14T06:07:11.000398Z",
     "iopub.status.idle": "2024-07-14T06:07:11.197707Z",
     "shell.execute_reply": "2024-07-14T06:07:11.196834Z",
     "shell.execute_reply.started": "2024-07-14T06:07:11.000729Z"
    }
   },
   "outputs": [],
   "source": [
    "bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T06:07:13.690786Z",
     "iopub.status.busy": "2024-07-14T06:07:13.690394Z",
     "iopub.status.idle": "2024-07-14T06:07:13.701932Z",
     "shell.execute_reply": "2024-07-14T06:07:13.700892Z",
     "shell.execute_reply.started": "2024-07-14T06:07:13.690757Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "max_seq_length = 512  # Set your desired maximum sequence length for BERT\n",
    "\n",
    "# Define the pre-processing transformations for images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "class MyMultimodalDataset(Dataset):\n",
    "    def __init__(self, data, transform=None, tokenizer=None, max_seq_length=512):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data.iloc[idx]['Image_Path']\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transform is not None:\n",
    "                image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image at index {idx}: {e}\")\n",
    "            return None, None, None, None\n",
    "\n",
    "        if image is None:\n",
    "            return None, None, None, None\n",
    "\n",
    "        context = self.data.iloc[idx]['context']\n",
    "\n",
    "        inputs = self.tokenizer(context, padding='max_length', truncation=True, max_length=self.max_seq_length, return_tensors='pt')\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        return image, input_ids, attention_mask, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T06:07:16.537841Z",
     "iopub.status.busy": "2024-07-14T06:07:16.537110Z",
     "iopub.status.idle": "2024-07-14T06:07:16.552215Z",
     "shell.execute_reply": "2024-07-14T06:07:16.551127Z",
     "shell.execute_reply.started": "2024-07-14T06:07:16.537809Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create custom datasets with MyMultimodalDataset\n",
    "train_dataset = MyMultimodalDataset(train_df, transform=transform, tokenizer=bert_tokenizer, max_seq_length=max_seq_length)\n",
    "test_dataset = MyMultimodalDataset(test_df, transform=transform, tokenizer=bert_tokenizer, max_seq_length=max_seq_length)\n",
    "val_dataset = MyMultimodalDataset(validation_df, transform=transform, tokenizer=bert_tokenizer, max_seq_length=max_seq_length)\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 8  # Set your desired batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T06:08:49.822472Z",
     "iopub.status.busy": "2024-07-14T06:08:49.822107Z",
     "iopub.status.idle": "2024-07-14T06:08:51.444044Z",
     "shell.execute_reply": "2024-07-14T06:08:51.443102Z",
     "shell.execute_reply.started": "2024-07-14T06:08:49.822438Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from transformers import ViTModel, AutoTokenizer, DistilBertModel, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Assuming you have defined your train_loader, val_loader, optimizer, criterion, model, bert_model, etc.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Feature dimensions from ViTModel and DistilBertModel\n",
    "img_feat_size = 7 * 7  # Flattened size\n",
    "text_feat_size = 768\n",
    "combined_input_dim = img_feat_size + text_feat_size\n",
    "\n",
    "# Define the regressor model outside the training loop\n",
    "regressor = torch.nn.Sequential(\n",
    "    torch.nn.Linear(combined_input_dim, 512),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.5),\n",
    "    torch.nn.Linear(512, 8)  # Change output size to match your number of labels\n",
    ").to(device)\n",
    "\n",
    "# Combine parameters from both models and the regressor for the optimizer\n",
    "optimizer = torch.optim.AdamW(regressor.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 1\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training time\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    regressor.train()\n",
    "\n",
    "    for images, texts, attention_masks, labels in tqdm(train_loader, desc='Training', leave=False):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        input_ids = texts.squeeze(1).to(device)\n",
    "        attention_mask = attention_masks.squeeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_image = model(pixel_values=images)\n",
    "        img_feats = outputs_image.last_hidden_state[:, 0, :].view(images.size(0), -1)  # Flatten image features\n",
    "        #print(f'Image features shape: {img_feats.shape}')  # Debug print\n",
    "\n",
    "        outputs_text = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_feats = outputs_text.last_hidden_state[:, 0, :]\n",
    "        #print(f'Text features shape: {text_feats.shape}')  # Debug print\n",
    "\n",
    "        combined_feats = torch.cat((img_feats, text_feats), dim=1)\n",
    "        #print(f'Combined features shape: {combined_feats.shape}')  # Debug print\n",
    "\n",
    "        predictions = regressor(combined_feats)\n",
    "        loss = criterion(predictions, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    epoch_train_loss = running_train_loss / len(train_loader)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    regressor.eval()\n",
    "\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_texts, val_attention_masks, val_labels in tqdm(val_loader, desc='val', leave=False):\n",
    "            val_images = val_images.to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "\n",
    "            val_input_ids = val_texts.squeeze(1).to(device)\n",
    "            val_attention_mask = val_attention_masks.squeeze(1).to(device)\n",
    "\n",
    "            outputs_image = model(pixel_values=val_images)\n",
    "            val_img_feats = outputs_image.last_hidden_state[:, 0, :].view(val_images.size(0), -1)  # Flatten image features\n",
    "\n",
    "            outputs_text = bert_model(input_ids=val_input_ids, attention_mask=val_attention_mask)\n",
    "            val_text_feats = outputs_text.last_hidden_state[:, 0, :]\n",
    "\n",
    "            val_combined_feats = torch.cat((val_img_feats, val_text_feats), dim=1)\n",
    "\n",
    "            val_predictions = regressor(val_combined_feats)\n",
    "            val_loss = criterion(val_predictions, val_labels)\n",
    "\n",
    "            running_val_loss += val_loss.item()\n",
    "\n",
    "    epoch_val_loss = running_val_loss / len(val_loader)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - \"\n",
    "          f\"Train Loss: {epoch_train_loss:.4f}, \"\n",
    "          f\"Val Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time: {execution_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T06:09:45.948645Z",
     "iopub.status.busy": "2024-07-14T06:09:45.947924Z",
     "iopub.status.idle": "2024-07-14T06:09:46.262731Z",
     "shell.execute_reply": "2024-07-14T06:09:46.261829Z",
     "shell.execute_reply.started": "2024-07-14T06:09:45.948615Z"
    }
   },
   "outputs": [],
   "source": [
    "# Testing phase\n",
    "regressor.eval()\n",
    "test_labels = []\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_images, test_texts, test_attention_masks, test_labels_batch in tqdm(test_loader, desc='Testing', leave=False):\n",
    "        test_images = test_images.to(device)\n",
    "        test_labels_batch = test_labels_batch.to(device)\n",
    "\n",
    "        test_input_ids = test_texts.squeeze(1).to(device)\n",
    "        test_attention_mask = test_attention_masks.squeeze(1).to(device)\n",
    "\n",
    "        outputs_image = model(pixel_values=test_images)\n",
    "        test_img_feats = outputs_image.last_hidden_state[:, 0, :].view(test_images.size(0), -1)  # Flatten image features\n",
    "\n",
    "        outputs_text = bert_model(input_ids=test_input_ids, attention_mask=test_attention_mask)\n",
    "        test_text_feats = outputs_text.last_hidden_state[:, 0, :]\n",
    "\n",
    "        test_combined_feats = torch.cat((test_img_feats, test_text_feats), dim=1)\n",
    "\n",
    "        test_predictions_batch = regressor(test_combined_feats)\n",
    "        _, test_predicted_labels = torch.max(test_predictions_batch, 1)\n",
    "\n",
    "        test_labels.extend(test_labels_batch.cpu().numpy())\n",
    "        test_predictions.extend(test_predicted_labels.cpu().numpy())\n",
    "\n",
    "# Compute and print metrics\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "precision = precision_score(test_labels, test_predictions, average='weighted')\n",
    "recall = recall_score(test_labels, test_predictions, average='weighted')\n",
    "f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "class_report = classification_report(test_labels, test_predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "print(f\"Total execution time for testing: {time.time() - end_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T06:09:50.236564Z",
     "iopub.status.busy": "2024-07-14T06:09:50.236165Z",
     "iopub.status.idle": "2024-07-14T06:09:50.242987Z",
     "shell.execute_reply": "2024-07-14T06:09:50.242026Z",
     "shell.execute_reply.started": "2024-07-14T06:09:50.236533Z"
    }
   },
   "outputs": [],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T06:09:52.276964Z",
     "iopub.status.busy": "2024-07-14T06:09:52.275966Z",
     "iopub.status.idle": "2024-07-14T06:09:52.283374Z",
     "shell.execute_reply": "2024-07-14T06:09:52.282290Z",
     "shell.execute_reply.started": "2024-07-14T06:09:52.276928Z"
    }
   },
   "outputs": [],
   "source": [
    "test_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T06:09:53.749461Z",
     "iopub.status.busy": "2024-07-14T06:09:53.749071Z",
     "iopub.status.idle": "2024-07-14T06:09:53.765453Z",
     "shell.execute_reply": "2024-07-14T06:09:53.764442Z",
     "shell.execute_reply.started": "2024-07-14T06:09:53.749428Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, mean_squared_error, classification_report\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "# Calculate precision, recall, F1-score overall (macro average)\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(test_labels, test_predictions, average='macro')\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(test_labels, test_predictions)\n",
    "\n",
    "# Calculate Sensitivity (Recall) for each class\n",
    "sensitivity_per_class = recall\n",
    "\n",
    "# Calculate Specificity for each class\n",
    "specificity_per_class = []\n",
    "for i in range(len(conf_matrix)):\n",
    "    tn = np.sum(conf_matrix) - (np.sum(conf_matrix[i, :]) + np.sum(conf_matrix[:, i]) - conf_matrix[i, i])\n",
    "    fp = np.sum(conf_matrix[:, i]) - conf_matrix[i, i]\n",
    "    specificity_per_class.append(tn / (tn + fp))\n",
    "\n",
    "# Print overall calculated metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision (macro): {precision}\")\n",
    "print(f\"Recall (macro): {recall}\")\n",
    "print(f\"F1-Score (macro): {f1_score}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Print Sensitivity and Specificity for each class\n",
    "print(f\"Sensitivity (Recall) for each class: {sensitivity_per_class}\")\n",
    "print(f\"Specificity for each class: {specificity_per_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5384900,
     "sourceId": 8948385,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
